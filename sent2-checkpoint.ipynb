{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00f98b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "33743ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(tweet: str):\n",
    "    \n",
    "    # Remove Leading Blank Spaces\n",
    "    tweet = tweet.strip()\n",
    "    \n",
    "    # Lower Case\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove URLS \n",
    "    url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    tweet = re.sub(url_pattern, \"\", tweet)\n",
    "    \n",
    "    # Remove UserName\n",
    "    username_pattern = re.compile(r\"@\\w+\")\n",
    "    tweet = re.sub(username_pattern, \"\", tweet)\n",
    "    \n",
    "    # Remove Hashtags\n",
    "    hashtag_pattern = re.compile(r\"#\\w+\")\n",
    "    tweet = re.sub(hashtag_pattern, \"\", tweet)\n",
    "    \n",
    "    # Character normalization // todaaaaay -> today\n",
    "    tweet = re.sub(r\"([a-zA-Z])\\1{2,}\", r'\\1', tweet)\n",
    "    \n",
    "    # Remove Special Characters\n",
    "    tweet = re.sub(r'[^a-zA-Z\\s]', \"\", tweet)\n",
    "    \n",
    "    # Word Tokenizer\n",
    "    tweet = nltk.word_tokenize(tweet)\n",
    "    \n",
    "    # Remove Stop Words \n",
    "    stop_words = set([re.sub(r'[^a-zA-Z\\s]', \"\", word) for word in nltk.corpus.stopwords.words(\"english\")])\n",
    "    tweet = [word for word in tweet if word not in stop_words]\n",
    "    \n",
    "    # lemmatization\n",
    "    def get_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"N\": \"n\", \"V\": \"v\", \"R\": \"r\", \"J\": \"a\"}\n",
    "        return tag_dict.get(tag, \"n\")\n",
    "    \n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    tweet = [lemma.lemmatize(word, pos=get_pos(word)) for word in tweet]\n",
    "    \n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4691b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\",encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f4b3d2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def pre_processing_2(tweet: str):\n",
    "    \n",
    "    # Remove Leading Blank Spaces\n",
    "    tweet = tweet.strip()\n",
    "    \n",
    "    # Lower Case\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove URLS \n",
    "    url_pattern = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    tweet = re.sub(url_pattern, \"\", tweet)\n",
    "    \n",
    "    # Remove UserName\n",
    "    username_pattern = re.compile(r\"@\\w+\")\n",
    "    tweet = re.sub(username_pattern, \"\", tweet)\n",
    "    \n",
    "    # Remove Hashtags\n",
    "    hashtag_pattern = re.compile(r\"#\\w+\")\n",
    "    tweet = re.sub(hashtag_pattern, \"\", tweet)\n",
    "    \n",
    "    # Character normalization // todaaaaay -> today\n",
    "    tweet = re.sub(r\"([a-zA-Z])\\1{2,}\", r'\\1', tweet)\n",
    "    \n",
    "    # Remove Special Characters\n",
    "    tweet = re.sub(r'[^a-zA-Z\\s]', \"\", tweet)\n",
    "    \n",
    "    # Word Tokenizer\n",
    "    tweet = nltk.word_tokenize(tweet)\n",
    "    \n",
    "    # Remove Stop Words \n",
    "    stop_words = set([re.sub(r'[^a-zA-Z\\s]', \"\", word) for word in nltk.corpus.stopwords.words(\"english\")])\n",
    "    tweet = [word for word in tweet if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    def get_pos(word):\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"N\": \"n\", \"V\": \"v\", \"R\": \"r\", \"J\": \"a\"}\n",
    "        return tag_dict.get(tag, \"n\")\n",
    "    \n",
    "    lemma = nltk.stem.WordNetLemmatizer()\n",
    "    tweet = [lemma.lemmatize(word, pos=get_pos(word)) for word in tweet]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    processed_tweet = ' '.join(tweet)\n",
    "    \n",
    "    return processed_tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cb0f6680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>38928346</td>\n",
       "      <td>652860.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Albania</td>\n",
       "      <td>2877797</td>\n",
       "      <td>27400.0</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>43851044</td>\n",
       "      <td>2381740.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>77265</td>\n",
       "      <td>470.0</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Angola</td>\n",
       "      <td>32866272</td>\n",
       "      <td>1246700.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Ghana</td>\n",
       "      <td>31072940</td>\n",
       "      <td>227540.0</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Greece</td>\n",
       "      <td>10423054</td>\n",
       "      <td>128900.0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Grenada</td>\n",
       "      <td>112523</td>\n",
       "      <td>340.0</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>Guatemala</td>\n",
       "      <td>17915568</td>\n",
       "      <td>107160.0</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>0-20</td>\n",
       "      <td>Guinea</td>\n",
       "      <td>13132795</td>\n",
       "      <td>246000.0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2      088c60f138                          my boss is bullying me...   \n",
       "3      9642c003ef                     what interview! leave me alone   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27479  ed167662a5                         But it was worth it  ****.   \n",
       "27480  6f7127d9d7     All this flirting going on - The ATG smiles...   \n",
       "\n",
       "                                           selected_text sentiment  \\\n",
       "0                    I`d have responded, if I were going   neutral   \n",
       "1                                               Sooo SAD  negative   \n",
       "2                                            bullying me  negative   \n",
       "3                                         leave me alone  negative   \n",
       "4                                          Sons of ****,  negative   \n",
       "...                                                  ...       ...   \n",
       "27476                                             d lost  negative   \n",
       "27477                                      , don`t force  negative   \n",
       "27478                          Yay good for both of you.  positive   \n",
       "27479                         But it was worth it  ****.  positive   \n",
       "27480  All this flirting going on - The ATG smiles. Y...   neutral   \n",
       "\n",
       "      Time of Tweet Age of User      Country  Population -2020  \\\n",
       "0           morning        0-20  Afghanistan          38928346   \n",
       "1              noon       21-30      Albania           2877797   \n",
       "2             night       31-45      Algeria          43851044   \n",
       "3           morning       46-60      Andorra             77265   \n",
       "4              noon       60-70       Angola          32866272   \n",
       "...             ...         ...          ...               ...   \n",
       "27476         night       31-45        Ghana          31072940   \n",
       "27477       morning       46-60       Greece          10423054   \n",
       "27478          noon       60-70      Grenada            112523   \n",
       "27479         night      70-100    Guatemala          17915568   \n",
       "27480       morning        0-20       Guinea          13132795   \n",
       "\n",
       "       Land Area (Km²)  Density (P/Km²)  \n",
       "0             652860.0               60  \n",
       "1              27400.0              105  \n",
       "2            2381740.0               18  \n",
       "3                470.0              164  \n",
       "4            1246700.0               26  \n",
       "...                ...              ...  \n",
       "27476         227540.0              137  \n",
       "27477         128900.0               81  \n",
       "27478            340.0              331  \n",
       "27479         107160.0              167  \n",
       "27480         246000.0               53  \n",
       "\n",
       "[27481 rows x 10 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e52f1df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=0.01, random_state=42)  # Sampling 10% of the data, you can adjust the fraction as needed\n",
    "\n",
    "# Reset index of the sampled DataFrame\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0980b6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Time of Tweet</th>\n",
       "      <th>Age of User</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population -2020</th>\n",
       "      <th>Land Area (Km²)</th>\n",
       "      <th>Density (P/Km²)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a7f72a928a</td>\n",
       "      <td>WOOOOOOOOOO   are you coming to Nottingham at...</td>\n",
       "      <td>t?  lovelovelove</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Libya</td>\n",
       "      <td>6871292</td>\n",
       "      <td>1759540.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ef42dee96c</td>\n",
       "      <td>resting had a whole day of walking</td>\n",
       "      <td>resting had a whole day of walking</td>\n",
       "      <td>neutral</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>59308690</td>\n",
       "      <td>1213090.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>07d17131b1</td>\n",
       "      <td>was in Palawan a couple of days ago, i`ll try ...</td>\n",
       "      <td>was in Palawan a couple of days ago, i`ll try ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>morning</td>\n",
       "      <td>46-60</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>4937786</td>\n",
       "      <td>68890.0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2820205db5</td>\n",
       "      <td>I know! I`m so slow its horrible. DON`T TELL ...</td>\n",
       "      <td>horrible.</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>10139177</td>\n",
       "      <td>82658.0</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7d3ce4363c</td>\n",
       "      <td>Glad I went out, glad I didn`t leave early, an...</td>\n",
       "      <td>glad</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>21-30</td>\n",
       "      <td>Montenegro</td>\n",
       "      <td>628066</td>\n",
       "      <td>13450.0</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>2e2619841e</td>\n",
       "      <td>hehe yeah! but too late hun</td>\n",
       "      <td>hehe yeah! but too late hun</td>\n",
       "      <td>neutral</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Guinea-Bissau</td>\n",
       "      <td>1968001</td>\n",
       "      <td>28120.0</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>b9c5b11c14</td>\n",
       "      <td>That sounds cool! And you`re paying - even be...</td>\n",
       "      <td>cool!</td>\n",
       "      <td>positive</td>\n",
       "      <td>night</td>\n",
       "      <td>31-45</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>84339067</td>\n",
       "      <td>770000.0</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>ecca496740</td>\n",
       "      <td>In worst case cenario, I`ll show up on OFFF wi...</td>\n",
       "      <td>worst</td>\n",
       "      <td>negative</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Switzerland</td>\n",
       "      <td>8654622</td>\n",
       "      <td>39500.0</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>9ff61fee3b</td>\n",
       "      <td>Glad to see  and more Red Bull Air Racers here...</td>\n",
       "      <td>Glad to see  and more Red Bull Air Racers here...</td>\n",
       "      <td>positive</td>\n",
       "      <td>noon</td>\n",
       "      <td>60-70</td>\n",
       "      <td>Tunisia</td>\n",
       "      <td>11818619</td>\n",
       "      <td>155360.0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>782cf9f227</td>\n",
       "      <td>Luke M here, thanks Ps Chris! Ur husband is p...</td>\n",
       "      <td>Luke M here, thanks Ps Chris! Ur husband is pr...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>night</td>\n",
       "      <td>70-100</td>\n",
       "      <td>Namibia</td>\n",
       "      <td>2540905</td>\n",
       "      <td>823000.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>275 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID                                               text  \\\n",
       "0    a7f72a928a   WOOOOOOOOOO   are you coming to Nottingham at...   \n",
       "1    ef42dee96c                 resting had a whole day of walking   \n",
       "2    07d17131b1  was in Palawan a couple of days ago, i`ll try ...   \n",
       "3    2820205db5   I know! I`m so slow its horrible. DON`T TELL ...   \n",
       "4    7d3ce4363c  Glad I went out, glad I didn`t leave early, an...   \n",
       "..          ...                                                ...   \n",
       "270  2e2619841e                        hehe yeah! but too late hun   \n",
       "271  b9c5b11c14   That sounds cool! And you`re paying - even be...   \n",
       "272  ecca496740  In worst case cenario, I`ll show up on OFFF wi...   \n",
       "273  9ff61fee3b  Glad to see  and more Red Bull Air Racers here...   \n",
       "274  782cf9f227   Luke M here, thanks Ps Chris! Ur husband is p...   \n",
       "\n",
       "                                         selected_text sentiment  \\\n",
       "0                                     t?  lovelovelove  positive   \n",
       "1                   resting had a whole day of walking   neutral   \n",
       "2    was in Palawan a couple of days ago, i`ll try ...   neutral   \n",
       "3                                            horrible.  negative   \n",
       "4                                                 glad  positive   \n",
       "..                                                 ...       ...   \n",
       "270                        hehe yeah! but too late hun   neutral   \n",
       "271                                              cool!  positive   \n",
       "272                                              worst  negative   \n",
       "273  Glad to see  and more Red Bull Air Racers here...  positive   \n",
       "274  Luke M here, thanks Ps Chris! Ur husband is pr...   neutral   \n",
       "\n",
       "    Time of Tweet Age of User        Country  Population -2020  \\\n",
       "0            noon       60-70          Libya           6871292   \n",
       "1           night      70-100   South Africa          59308690   \n",
       "2         morning       46-60        Ireland           4937786   \n",
       "3            noon       60-70     Azerbaijan          10139177   \n",
       "4            noon       21-30     Montenegro            628066   \n",
       "..            ...         ...            ...               ...   \n",
       "270          noon       60-70  Guinea-Bissau           1968001   \n",
       "271         night       31-45         Turkey          84339067   \n",
       "272          noon       60-70    Switzerland           8654622   \n",
       "273          noon       60-70        Tunisia          11818619   \n",
       "274         night      70-100        Namibia           2540905   \n",
       "\n",
       "     Land Area (Km²)  Density (P/Km²)  \n",
       "0          1759540.0                4  \n",
       "1          1213090.0               49  \n",
       "2            68890.0               72  \n",
       "3            82658.0              123  \n",
       "4            13450.0               47  \n",
       "..               ...              ...  \n",
       "270          28120.0               70  \n",
       "271         770000.0              110  \n",
       "272          39500.0              219  \n",
       "273         155360.0               76  \n",
       "274         823000.0                3  \n",
       "\n",
       "[275 rows x 10 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5470e3c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['processed_text'] = df['text'].apply(pre_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b39b225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            [wo, come, nottingham, point, lovelovelove]\n",
       "1                               [rest, whole, day, walk]\n",
       "2      [palawan, couple, day, ago, ill, try, post, pi...\n",
       "3                       [know, im, slow, horrible, tell]\n",
       "4      [glad, go, glad, leave, early, glad, afterpart...\n",
       "                             ...                        \n",
       "270                              [hehe, yeah, late, hun]\n",
       "271                       [sound, cool, pay, even, well]\n",
       "272    [bad, case, cenario, ill, show, white, mask, s...\n",
       "273    [glad, see, red, bull, air, racer, keep, u, loop]\n",
       "274    [luke, thanks, p, chris, ur, husband, pretty, ...\n",
       "Name: processed_text, Length: 275, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['processed_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "055ce22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size : 900\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[\"processed_text\"].values, df[\"sentiment\"].values, train_size=0.8)\n",
    "\n",
    "# Building the vocabulary\n",
    "vocab = set()\n",
    "for text in x_train:\n",
    "    for word in text:\n",
    "        vocab.add(word)\n",
    "print(\"Vocab Size :\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00db29ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63468, 784500)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "g_model = Word2Vec(vector_size=200, window=5, workers=5)\n",
    "g_model.build_vocab(x_train)\n",
    "g_model.train(x_train, total_examples=g_model.corpus_count, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f26f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_vocab(word_l):\n",
    "    for word in word_l:\n",
    "        if word not in g_model.wv:\n",
    "            return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "train_vec = [g_model.wv[x].sum(axis = 0) if len(x) and in_vocab(x) else np.zeros((200)) for x in x_train]\n",
    "test_vec  = [g_model.wv[x].sum(axis = 0) if len(x) and in_vocab(x) else np.zeros((200)) for x in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e3e12a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3c67b2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.36363636363636365\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        18\n",
      "     neutral       0.36      1.00      0.53        20\n",
      "    positive       0.00      0.00      0.00        17\n",
      "\n",
      "    accuracy                           0.36        55\n",
      "   macro avg       0.12      0.33      0.18        55\n",
      "weighted avg       0.13      0.36      0.19        55\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(train_vec, y_train)\n",
    "\n",
    "# Predicting on test data\n",
    "predict = model.predict(test_vec)\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"Accuracy Score :\", accuracy_score(y_test, predict), end='\\n\\n')\n",
    "print(classification_report(y_true=y_test, y_pred=predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "195aaf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_offensive(text):\n",
    "    processed_text = pre_processing(text)\n",
    "    word_list = nltk.word_tokenize(processed_text)\n",
    "    vector = [g_model.wv[word] for word in word_list if word in g_model.wv]\n",
    "    if not vector:\n",
    "        print(\"Clean sentence.\")\n",
    "        return\n",
    "    vector_mean = sum(vector) / len(vector)\n",
    "    prediction = model.predict([vector_mean])\n",
    "    if prediction[0] == 1:\n",
    "        print(\"Hate speech or offensive language detected.\")\n",
    "    else:\n",
    "        print(\"Clean sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "59e8b3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: hello\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [74], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m input_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter a sentence: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mpredict_offensive\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sentence\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [73], line 3\u001b[0m, in \u001b[0;36mpredict_offensive\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_offensive\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     processed_text \u001b[38;5;241m=\u001b[39m pre_processing(text)\n\u001b[1;32m----> 3\u001b[0m     word_list \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     vector \u001b[38;5;241m=\u001b[39m [g_model\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_list \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m g_model\u001b[38;5;241m.\u001b[39mwv]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vector:\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1328\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1460\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[0;32m   1430\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1433\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1394\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1396\u001b[0m \n\u001b[0;32m   1397\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[0;32m   1399\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "input_sentence = input(\"Enter a sentence: \")\n",
    "\n",
    "# Predict\n",
    "predict_offensive(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "928c991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace_negative_words(text):\n",
    "#     negative_keywords = extract_negative_keywords(text)\n",
    "#     positive_replacements = {'hate': 'love', 'terrible': 'excellent', 'negative_word_3': 'positive_word_3'}\n",
    "#     replaced_text = text\n",
    "#     for word in negative_keywords:\n",
    "#         if word in positive_replacements:\n",
    "#             replaced_text = replaced_text.replace(word, positive_replacements[word])\n",
    "#     return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4c353f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e87639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_negative_keywords(text):\n",
    "#     r = Rake()\n",
    "#     r.extract_keywords_from_text(text)\n",
    "#     return r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35e6b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_offensive_2(text):\n",
    "    processed_text = pre_processing(text)\n",
    "    word_list = nltk.word_tokenize(processed_text)\n",
    "    vector = [g_model.wv[word] for word in word_list if word in g_model.wv]\n",
    "    if not vector:\n",
    "        print(\"Clean sentence.\")\n",
    "        return\n",
    "    vector_mean = sum(vector) / len(vector)\n",
    "    prediction = model.predict([vector_mean])\n",
    "    if prediction[0] == 1:\n",
    "        print(\"Hate speech or offensive language detected.\")\n",
    "        replaced_text = replace_negative_words(text)\n",
    "        print(\"Replaced sentence:\", replaced_text)\n",
    "    else:\n",
    "        print(\"Clean sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7f883ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: i love you\n"
     ]
    }
   ],
   "source": [
    "input_sentence = input(\"Enter a sentence: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7242135",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mpredict_offensive_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sentence\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [26], line 3\u001b[0m, in \u001b[0;36mpredict_offensive_2\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_offensive_2\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     processed_text \u001b[38;5;241m=\u001b[39m pre_processing(text)\n\u001b[1;32m----> 3\u001b[0m     word_list \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     vector \u001b[38;5;241m=\u001b[39m [g_model\u001b[38;5;241m.\u001b[39mwv[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_list \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m g_model\u001b[38;5;241m.\u001b[39mwv]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vector:\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1328\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1460\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n\u001b[0;32m   1430\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1433\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32mc:\\users\\adith\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1394\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1396\u001b[0m \n\u001b[0;32m   1397\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n\u001b[0;32m   1399\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "predict_offensive_2(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f88ca678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\adith\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94b39ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import random\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "498ff0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: i hate you bitch die\n"
     ]
    }
   ],
   "source": [
    "def predict_offensive_3(text):\n",
    "    processed_text = preprocess_text(text)\n",
    "    word_list = nltk.word_tokenize(processed_text)\n",
    "    vector = [g_model.wv[word] for word in word_list if word in g_model.wv]\n",
    "    if not vector:\n",
    "        print(\"Clean sentence.\")\n",
    "        return\n",
    "    vector_mean = sum(vector) / len(vector)\n",
    "    prediction = model.predict([vector_mean])\n",
    "    if prediction[0] == 1:\n",
    "        print(\"Hate speech or offensive language detected.\")\n",
    "        replaced_text = replace_negative_words(text)\n",
    "        print(\"Replaced sentence:\", replaced_text)\n",
    "    else:\n",
    "        print(\"Clean sentence.\")\n",
    "\n",
    "# Function to replace negative words with positive ones\n",
    "def replace_negative_words(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    replaced_text = text\n",
    "    for i, word in enumerate(word_list):\n",
    "        if is_negative_word(word):\n",
    "            positive_word = get_random_positive_word()\n",
    "            replaced_text = replaced_text.replace(word, positive_word, 1)  # Replace only the first occurrence\n",
    "    return replaced_text\n",
    "\n",
    "# Function to check if a word is negative using VADER sentiment analysis\n",
    "def is_negative_word(word):\n",
    "    # Use VADER to get the sentiment score of the word\n",
    "    compound_score = sia.polarity_scores(word)['compound']\n",
    "    return compound_score < 0  # Negative compound score indicates negative sentiment\n",
    "\n",
    "# Function to get a random positive word\n",
    "def get_random_positive_word():\n",
    "    positive_words = ['love', 'excellent', 'amazing', 'joyful', 'wonderful']  # Add more positive words if needed\n",
    "    return random.choice(positive_words)\n",
    "\n",
    "# Take input sentence\n",
    "input_sentence = input(\"Enter a sentence: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e40f02d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hate speech or offensive language detected.\n",
      "Replaced sentence: i amazing you love wonderful\n"
     ]
    }
   ],
   "source": [
    "predict_offensive_3(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6b740697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: hello wow\n",
      "Input type: <class 'str'>\n",
      "Processed text: hello wow\n",
      "Clean sentence.\n"
     ]
    }
   ],
   "source": [
    "def predict_offensive(text):\n",
    "    print(\"Input type:\", type(text))  # Debug line to check input type\n",
    "    if isinstance(text, str):  # Check if the input is a string\n",
    "        processed_text = pre_processing_2(text)\n",
    "        print(\"Processed text:\", processed_text)  # Debug line to check processed text\n",
    "        word_list = nltk.word_tokenize(processed_text)\n",
    "        vector = [g_model.wv[word] for word in word_list if word in g_model.wv]\n",
    "        if not vector:\n",
    "            print(\"Clean sentence.\")\n",
    "            return\n",
    "        vector_mean = sum(vector) / len(vector)\n",
    "        prediction = model.predict([vector_mean])\n",
    "        if prediction[0] == 1:\n",
    "            print(\"Hate speech or offensive language detected.\")\n",
    "        else:\n",
    "            print(\"Clean sentence.\")\n",
    "    else:\n",
    "        print(\"Invalid input. Please provide a valid string.\")\n",
    "\n",
    "# Take input sentence\n",
    "input_sentence = input(\"Enter a sentence: \")\n",
    "predict_offensive(input_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e979023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
